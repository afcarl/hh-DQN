\documentclass{article}
\usepackage{algorithm}
\usepackage{algorithmic}
\begin{document}
\title{Hierarchy Deep Q-Learning Framework}
\maketitle
At first, we construct a hierarchy framework without deep neural network.
After we specify all tasks, the set containing all tasks is defined as $T:\{T_1,T_2,T_3...\}$. We also define the available action set as $A:\{a_1,a_2,a_3...\}$. We define $O:T\cup A$, which is often used as all the operation we can perform. Besides, we define a global task named $global$, which only focus on getting more extrinsic reward during the game. We define $G:T\cup \{global\}$, which is often used as the super-task we are going through. So a task can be regarded as an operation as well as a goal. $\Omega$ means the state set. 

Then, we define tasks as follows(containing actions as primitive tasks):
\begin{eqnarray}
O:\{& \pi : \Omega * O\rightarrow [0,1],&\\
     \nonumber & \beta : \Omega \rightarrow [0,1]&\}
\end{eqnarray}

The $\pi$ specifies the policy: $\pi_o(s,t)$ mean the policy of $o$: the probability of taking task $t$ under the state $s$. and $\beta$ defines the termination condition of a task, $\beta_o(s)$ denotes the probability of terminating the task at the state of $s$, meaning the goal of it.
One thing notable is that primitive actions are also compatible for the subtask definition, while the policy $\pi(s,o)=1$ for all states only when $o$ is the current action and $\beta(s)=1$ for all states, meaning the task will definitely stop after one action.

Now we can define the learning procedure.

\section{Hierarchy Model Learning}
We have to define different intrinsic reward based on the task on which the subtask is performed.

$r^a_s \quad a\in{A},s\in{\Omega}$ : extrinsic reward gained when perform primitive action $a$ on the state of $s$, it's given by the game model.

Because actions can be regarded as primitive tasks, we can define a more general reward:

$r^o_s \quad o\in{O},s\in{\Omega}$ : extrinsic reward gained when perform task $o$ on the state of $s$.

Then we define the mixture reward under the hierarchy reinforcement learning:

$r^o_{s,g} \quad o\in{O},s\in{\Omega},g\in{G}$ : mixture reward gained when perform subtask $o$ under the task goal of $g$ on the state of $s$.

Then we define the Bellman equation of reward and the intrinsic part of reward:

\begin{equation}\label{reward_function}
r^o_s=\sum_{t\in{O}}{\pi_o(s,t)E[r^t_s + \gamma^k(1-\beta_o(s'))r^o_{s'})]}
\end{equation}
In the equation, $k$ is the number of steps used performing $t$ from state $s$. The reward for task $o$ under state $s$ is the expectation of the sum of the accumulate reward gained during the process of the first subtask $t$ and the future expected reward if the task is not terminated according to $\beta_o(s')$. Primitive action also satisfies this equation, but will be given directly from the model.

\begin{eqnarray}\label{reward_function2}
& r^o_{s,g}=\sum_{t\in{O}}{\pi_o(s,t)E[IR(g,s')+r^t_s + \gamma^k(1-\beta_o(s'))r^o_{s'}]} \\
& IR(g,s) =\rho * \beta_g{(s)}
\end{eqnarray}

We define the mixture reward with two parts: getting more rewards while trying to reach specified states. Parameter $\rho$ is used to control the relative weight of the local goal of task $g$. Under the reward model, we can construct the value learning procedure. One exception is that when $g==global$, $IR(g,s)=0$, we only consider maximum extrinsic reward.

This procedure helps building the model of the current hierarchy structure.

\section{\label{sub:HVL}Hierarchy Value Learning}
In the value learning procedure of hierarchy tasks. We define a new Q-function. $Q^*(g,s,o) \quad g\in G,s\in \Omega, o\in O$ denotes the max Q-value after performing task $o$ from the state $s$ under the goal of task $g$, if from that time on we always conduct the optimal operations.

We get the Bellman equation of this new Q-value as follows:
\begin{equation}\label{Q_function}
Q^*(g,s,o) = \sum_{t\in O}{\pi_o(s,t)E[r^t_{(s,g)}+\gamma^k((1-\beta_o(s'))Q^*(g,s',o)+\beta_o(s')\max \limits_{t'\in O}Q^*(g,s',t'))]}
\end{equation}
$k$ is the number of primitive steps used for task $t$ to finish from state $s$. We notice that this way of learning can be done just after every finishing of subtask of $o$. It also satisfied when $g==global$.

This procedure get the Q-value from the current hierarchy structure, which will also dynamically change the policy of tasks to building better hierarchy structure.

\section{$\beta$ learning} \label{beta_learning}
The goal of random initialized tasks should also be learned to adapt the goal to maximum Q-value in section \ref{sub:HVL}. We use interrupting policy to help learning the $\beta$ function as well as choose better policy.

In the running phase, when $Q^*(g,s,currento)<\max \limits_{t\in O}{Q^*(g,s,t)}$, then interrupt the task $currento$ to choose task $\mathop{argmax} \limits_{t\in O}{Q^*(g,s,t)}$ with probability $sigmoid(\lambda * (\max \limits_{t'\in O}Q^*(g,s',t')-Q^*(g,s',o)))$. $\lambda$ controls the balance between keeping the current policy or changing to better policy.\cite{sutton1999between} has proved this policy will definitely improve (at least not deteriorate) V-value. Meanwhile, the $\beta$ of is updated. The update way is inspired by the new Q-function. The first derivative of $Q^*(g,s,o)$ with respect to $\beta_o(s')$ from equation \ref{Q_function} is as follows:
\begin{equation}
\frac{\partial Q^*(g,s,o)}{\partial \beta_o(s')}=\sum_{t\in O}{\pi_o(s,t)E[\gamma^k(\max \limits_{t'\in O}Q^*(g,s',t')-Q^*(g,s',o))]}
\end{equation}
To maximum Q value we take use of the equation. When a subtask $t$ is finished, we accumulate the grad value $grad'=grad+(\max \limits_{t'\in O}Q^*(g,s',t')-Q^*(g,s',o)))$, Then:
\begin{equation}
\beta_o(s)=sigmoid(grad)
\end{equation}

This procedure helps building better hierarchy structure by changing the goal of tasks.

\section{Deep Learning Procedure}
\begin{algorithm}[t]
\caption{Deep Hierarchy Reinforcement Learning Procedure}
\label{alg:framework}
\begin{algorithmic}[1]
\STATE Update the reward with Bellman equation of reward (equation \ref{reward_function}).\\
$r^o_s \leftarrow r^o_s+\alpha[r^t_s+\gamma^k(1-\beta_o(s'))r^o_{s'})-r^o_s]$
% why not use r(*,s)?...because it needs policy of every task, and will learn many nonsense: small probability event.
\STATE Update the Q-value with Bellman equation of Q-value.\\
$Q^*(g,s,o) \leftarrow Q^*(g,s,o) + \alpha[r^t_{(s,g)}+\gamma^kU(g,s',o)-Q^*(g,s,o)]$\\
where, \\
$U(g,s,o)=(1-\beta_o(s))Q^*(g,s,o)+\beta_o(s)\max \limits_{t'\in O}Q^*(g,s,t')$       
\STATE Consider if the subgoal of task $o$ is finished by $\beta_o(s)$.
\STATE Consider whether interrupting, if so, update the $\beta_o(s)$ as described in Section \ref{beta_learning}.
\end{algorithmic}
\end{algorithm}

Predefine the number of tasks as $n$.
We may need three DNNs: one for reward mapping $r(s,o)$, one for Q-value mapping $Q^*(g,s,o)$ and one for $\beta(s,o)$. To calculate $\max \limits_{t'\in O}Q^*(g,s',t')$ in one run of NN, the $o$ parameter will be fixed in the output of the NN as in \cite{mnih2015human-level}. To make $g$ as a input of NN of Q-value, we will assign every task a distributed vector as the input. The structure of NN has not decided. 

During the reinforcement learning, the model will keep finding the next task to perform based on the $\pi_o(s,t)$, we use a probabilistic definition of it.

\begin{equation}\label{policy_function}
\pi_o(s,t)=\frac{exp(\mu*Q^*(o,s,t))}{\sum_{t'\in o}{exp(\mu*Q^*(o,s,t'))}}
\end{equation}

The parameter $\mu$ is used to control the balance between the max-Q policy and softmax-Q policy.

When one subtask $t$ is finished from the state $s$ to $s'$ under the super-task and current-task $(g,o)$, learning procedure begins as in Algorithm \ref{alg:framework}.

There are 3 new parameters in learning procedure : $\rho, \lambda, \mu$, their use are explained in the text. $\rho$ should decrease during the learning procedure, gradually making learning focus on extrinsic reward rather than intrinsic reward. $\lambda$ should gradually increase, because when Q-value is sufficiently learned, the interrupting policy will be more credible. $\mu$ should gradually increase, for the same reason for $\lambda$, task policy should tend to be more deterministic to max-Q policy.

\bibliographystyle{plain}
\bibliography{all}

\end{document}

